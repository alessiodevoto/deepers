{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVYJg6pGEaS0"
      },
      "source": [
        "# Deepcamp: Codelab 4\n",
        "\n",
        "**In this tutorial we will cover**:\n",
        "\n",
        "- Large Language Models (ever heard of ChatGPT ðŸ‘€ ?)\n",
        "- LangChain: a tool to allow interaction between LLMs\n",
        "\n",
        "\n",
        "**Author**:\n",
        "- Alessio Devoto (alessio.devoto@uniroma1.it)\n",
        "\n",
        "\n",
        "**Duration**: 50 mins \n",
        "\n",
        "\n",
        "ðŸ›‘ **Warning**: Make sure you have an OpenAI token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIOwSLhQ0a05"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjhb8WWF9toT"
      },
      "outputs": [],
      "source": [
        "# usual general imports \n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYEswD8hEcaA"
      },
      "source": [
        "# LangChain ðŸ¦œâ›“\n",
        "\n",
        "[Langchain](https://python.langchain.com/en/latest/index.html) makes interaction with Large Language Models easy and intuitive.\n",
        "\n",
        "The whole library is comprised of the following building blocks:\n",
        "\n",
        "- **models**: a model represents an llm or llm-related service offered by an API endpoint\n",
        "- **prompts**: a prompt is what we feed as input to the model\n",
        "- **indexes**: an index is a set of rules to split, embed and store documents so that language models can best interact with them\n",
        "- **chains**: a sequence of modular components (models, prompts, other chains ...)  combined in a particular way \n",
        "\n",
        "The library also offers other modules that we are not interested in today: Agents and Memory. Please refer to the [LangChain official](https://docs.langchain.com/docs/) guide for those.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Zl1r5eZKhz"
      },
      "source": [
        "## Models\n",
        "\n",
        "Let us start by exploring a simple Langchain Model.\n",
        "- Model allows us to interact with a number of LLMs providers with a uniform interface. \n",
        "- Check the list of supported LLMs [here](https://python.langchain.com/en/latest/modules/models/llms/integrations.html)\n",
        "\n",
        "We are going to use OpenAI's API, which exposes a lot of [versions](https://platform.openai.com/docs/models/gpt-4) of ChatGPT. \n",
        "\n",
        "To do so, you must have an OpenAI account and generate an [access key](https://platform.openai.com/account/api-keys).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIeaJRs9aMfo"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\" # your api key here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l3f67UF6zGj"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    model_name=\"text-davinci-002\",    # you can pick other models but this is the best speed/effectiveness tradeoff                     \n",
        "    temperature=0                     # for OpenAi, see options here: https://platform.openai.com/docs/api-reference/completions/create\n",
        "    ) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59fR5lmC68sJ",
        "outputId": "49885464-473f-4841-f93b-ae9357348a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "There is no one-size-fits-all answer to this question, as the best way to learn about deep learning and AI will vary depending on your level of expertise and experience. However, some suggestions for how to learn about deep learning and AI include attending conferences and workshops, reading books and articles on the topic, and taking online courses.\n"
          ]
        }
      ],
      "source": [
        "# let's ask something to the language model\n",
        "\n",
        "llm_result = llm(\"What should I do to learn about Deep Learning and AI ?\")\n",
        "print(llm_result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Try a couple of times with a different temperature and top-k! Are you getting the same answer?\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    model_name=\"text-davinci-002\",    # you can pick other models but this is the fastest one                     \n",
        "    temperature=.4                     # for OpenAi, see options here: https://platform.openai.com/docs/api-reference/completions/create\n",
        "    ) \n",
        "\n",
        "llm_result = llm(\"What should I do to learn about Deep Learning and AI ?\")\n",
        "print(llm_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUfTdppTZoq_",
        "outputId": "0853e95e-24e0-4dac-dbbc-22b2b2ff6626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "There are a number of ways to learn about deep learning and AI. One way is to attend conferences and workshops on the topic. Another way is to read books and articles about deep learning and AI. Finally, there are online courses available that can teach you about deep learning and AI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The stochasticity in the answer is due to the different temperature we used. \n",
        "\n",
        "Large language models output *a probability for each possible token in their vocabulary* (we can regard tokens as words in this case). In the **decoding step** we decide how to use that probability to choose the next token. \n",
        "\n",
        "Instead of always picking the token with highest probability (greedy decoding) we can decode using a number of different approaches to perform an 'exploration' op possible outcomes (top-k, top-p, temperature).\n",
        "\n",
        "With temperature, we are basically affecting the model's prediction [confidence](https://lukesalamone.github.io/posts/what-is-temperature/) over next tokens,"
      ],
      "metadata": {
        "id": "Rqt0jQaCZ9VJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ1sm5xxa0y5"
      },
      "source": [
        "## Prompts\n",
        "\n",
        "A prompt is a nice way to format the input before it is actually fed to the model. \n",
        "\n",
        "This can be useful in case we want to keep part of the prompt hidden from the user but still provide an optimal response.\n",
        "\n",
        "Let us ask a simple question to the LLM. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(\n",
        "    model_name=\"text-davinci-002\",                        \n",
        "    temperature=0                     \n",
        "    ) "
      ],
      "metadata": {
        "id": "FC3CEuo6czTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDxZA_9Fug2r"
      },
      "outputs": [],
      "source": [
        "question =  \"There are 16 balls. Half of the balls are golf balls. Half of the golf balls are blue balls. How many blue golf balls are there?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IVYRPlLucBt",
        "outputId": "883e4ab8-53f9-4a00-963d-12975266ad06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "There are 8 blue golf balls.\n"
          ]
        }
      ],
      "source": [
        "llm_results = llm(question)\n",
        "print(llm_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXghfkJnuzD9"
      },
      "source": [
        "Ok, looks like it got it wrong... ðŸ¤“ Can we provide a better question, which can help the model give a [better answer](https://arxiv.org/pdf/2205.11916.pdf) ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znI6KaQhvJuP",
        "outputId": "e3d73cb8-8bcb-4b13-a428-1da883d863fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Prompt: \n",
            "There are 16 balls. Half of the balls are golf balls. Half of the golf balls are blue balls. How many blue golf balls are there?\n",
            "Let's think about this step by step\n",
            "\n",
            "LLM Output: \n",
            "There are 16 balls.\n",
            "\n",
            "There are 8 golf balls.\n",
            "\n",
            "There are 4 blue golf balls.\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "\n",
        "# question will be replaced by user's prompt\n",
        "template = \"\"\"\n",
        "{question}\n",
        "Let's think about this step by step\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(question=question) #look how we use the parameter\n",
        "\n",
        "print (f\"Final Prompt: {final_prompt}\")\n",
        "\n",
        "print (f\"LLM Output: {llm(final_prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise ðŸ‹: Make a prompt\n",
        "\n",
        "Create a prompt for getting touritic info about a city. The user will just type the city and will get information about the most important things to do in that city"
      ],
      "metadata": {
        "id": "Y6tkeC_wdIU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "ggFv5XM-dDkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Peek solution\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "What are the most intersting places I should visit in {city} ?\n",
        "Answer shortly.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"city\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(city='Rome')\n",
        "\n",
        "print (f\"Final Prompt: {final_prompt}\")\n",
        "\n",
        "print (f\"LLM Output: {llm(final_prompt)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2Lu_wvqdvMK",
        "outputId": "f3212f8e-e61e-49c4-a12e-cdf4cc42f414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Prompt: \n",
            "What are the most intersting places I should visit in Rome ?\n",
            "Answer shortly.\n",
            "\n",
            "LLM Output: \n",
            "There are a few interesting places to visit in Rome, such as the Colosseum, the Vatican, and the Sistine Chapel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMiFwuIXA2vU"
      },
      "source": [
        "## Chains\n",
        "\n",
        "Chains allow us to chain ðŸ˜ƒ modules and prompts in order to attain a task specific goal. \n",
        "There are several kinds of chains. We can identify two main categories:\n",
        "\n",
        "- LLM Chains: interaction of Model, Prompt and OutputParsers\n",
        "- Index-Related Chains: LLM chains to deal with Documents and External \"Memory\"\n",
        "\n",
        "In addition, we have a number of chains that allow you to interact in different fashions with all the modules in the library or even your own local machine processes (See later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFu8x4UDG8Y0"
      },
      "source": [
        "### LLM Chain\n",
        "\n",
        "A simple LLM Chain accepts two main input arguments:\n",
        "1. The LLM `Model` to be used\n",
        "2. The `Prompt` template to be used (if any)\n",
        "\n",
        "You can also add an output parser at the end of the chain to edit the chain's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaCd2ffuBfnB",
        "outputId": "83ab3ae4-5550-41b9-b32d-bc19c0d4946b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a sad poem about ducks in slang english.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Ducks is some real sad birds\n",
            "They just waddle around all day\n",
            "And every time they quack\n",
            "It just sound like they're sayin' \"fuck\"\n",
            "\n",
            "Life ain't easy for no ducks\n",
            "They got webbed feet and feathers\n",
            "And they always look so cold\n",
            "They just waddle around in the water\n",
            "And every time they try to fly\n",
            "They just end up fallin' down\n",
            "\n",
            "Ducks is some real sad birds\n",
            "And I can't help but feel bad\n",
            "For everything they go through\n",
            "They just try to live their lives\n",
            "The best way they know how\n",
            "And I hope one day they'll find\n",
            "A place where they can be happy\n"
          ]
        }
      ],
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "template = \"\"\"Write a {adjective} poem about {subject} in {language}.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\", \"language\"])\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt, \n",
        "    llm=llm, \n",
        "    verbose=True)\n",
        "\n",
        "results = llm_chain.run(adjective=\"sad\", subject=\"ducks\", language=\"slang english\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLX8q7wbIKrg",
        "outputId": "afdb5b13-24d1-4182-debc-8b5ff18f1f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a sad poem about ducks in italian.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "I piccoli anatroccoli\n",
            "\n",
            "nascono nel fango\n",
            "\n",
            "e si nutrono di insetti\n",
            "\n",
            "fino a quando non sono pronti\n",
            "\n",
            "per volare via\n",
            "\n",
            "lontano dal loro nido.\n",
            "\n",
            "Ma prima di allora\n",
            "\n",
            "i cacciatori vengono\n",
            "\n",
            "e uccidono i piccoli\n",
            "\n",
            "anatroccoli\n",
            "\n",
            "per farne il prezioso\n",
            "\n",
            "piumaggio.\n",
            "\n",
            "Ogni anno\n",
            "\n",
            "i piccoli anatroccoli\n",
            "\n",
            "muoiono\n",
            "\n",
            "senza poter mai\n",
            "\n",
            "volare via.\n"
          ]
        }
      ],
      "source": [
        "# Just for fun\n",
        "\n",
        "results = llm_chain.run(adjective=\"sad\", subject=\"ducks\", language=\"italian\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_slrPMFI04t"
      },
      "source": [
        "### Exercise ðŸ‹: Bash Chain\n",
        "\n",
        "Allocate an LLM Bash chain which chains the LM with a bash process on your local machine. Then ask the llm to create a bash script which echoes `DeepCamp is so cool!` inside the shell.\n",
        "\n",
        "\n",
        "Follow the steps in the template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu-1d9DeK5Aj"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMBashChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# 1. Create the language model\n",
        "llm = \n",
        "\n",
        "# 2. Create the chain\n",
        "bash_chain = LLMBashChain(llm=)\n",
        "\n",
        "# 3. Create prompt and run the chain\n",
        "# You should be very clear about what you want the llm to do, as we are using an old version of GPT and it could misunderstand you \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "dgRmR2goImyL",
        "outputId": "813b77d6-7786-4444-84ac-1208313cfa16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
            "Please write a bash script that prints 'DeepCamp is so cool!' to the console.\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "```bash\n",
            "echo \"DeepCamp is so cool!\"\n",
            "```\u001b[0m['```bash', 'echo \"DeepCamp is so cool!\"', '```']\n",
            "\n",
            "Answer: \u001b[33;1m\u001b[1;3mDeepCamp is so cool!\n",
            "\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DeepCamp is so cool!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#@title Peek Solution ðŸ‘€\n",
        "\n",
        "from langchain.chains import LLMBashChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-002\", temperature=0)\n",
        "\n",
        "text = \"Please write a bash script that prints 'DeepCamp is so cool!' to the console.\"\n",
        "\n",
        "bash_chain = LLMBashChain(llm=llm, verbose=True)\n",
        "\n",
        "bash_chain.run(text) # this is reallt being executed in a bash process on your local machine! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p29qngHOL-tI"
      },
      "source": [
        "Now ask the chain to create a directory named `labs_deepcamp`, run it, and check if the dir was actually created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5mUXC6oMEk7"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "xsJai7oWKJtv",
        "outputId": "5e3dc5ee-edf2-40b3-969d-7126bd58c230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
            "Please write a bash script that creates a directory named 'labs_deepers'.\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "```bash\n",
            "mkdir labs_deepers\n",
            "```\u001b[0m['```bash', 'mkdir labs_deepers', '```']\n",
            "\n",
            "Answer: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "#@title Peek Solution ðŸ‘€\n",
        "\n",
        "text = \"Please write a bash script that creates a directory named 'labs_deepers'.\"\n",
        "\n",
        "bash_chain = LLMBashChain(llm=llm, verbose=True)\n",
        "\n",
        "bash_chain.run(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also combine single in - single out chains with `SimpleSequentialChain`, like this: \n",
        "\n",
        "```\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
        "```\n",
        "\n",
        " (We'll try this later ðŸ˜€)\n"
      ],
      "metadata": {
        "id": "3qDyatjWg2Y4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj1BNF21I0ms"
      },
      "source": [
        "## Indexes \n",
        "\n",
        "The concept of Index in LangChain is quite broad. Indexes refer to ways to structure documents so that LLMs can best interact with them.\n",
        "\n",
        "A naive way of combining LLMs and documents would be injecting the content of a document (assuming it is a text document) into the LLM as a simple prompt. This might work for small docs, as the may possibly fit the LLM [context](https://www.theatlantic.com/technology/archive/2023/03/gpt-4-has-memory-context-window/673426/). When we deal with larger or multiple documents though, this is not feasible. \n",
        "\n",
        "We have four classes composing the Index module:\n",
        "\n",
        "- **Document Loaders**: responsible for loading documents from various sources.\n",
        "\n",
        "- **Text Splitters**: responsible for splitting text into smaller chunks.\n",
        "\n",
        "- **VectorStores**: Databases of stored documents\n",
        "\n",
        "- **Retrievers**: nterface for fetching relevant documents to combine with language models.\n",
        "\n",
        "\n",
        "As a first step, let us download a txt document containing a famous speech by Elon Musk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SM0yaEON3Qr",
        "outputId": "2fce04e1-281c-448f-a8e3-704e5e4da7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-26 09:47:49--  https://raw.githubusercontent.com/alessiodevoto/deepers/main/data/elon_musk_speech.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10139 (9.9K) [text/plain]\n",
            "Saving to: â€˜elon_musk_speech.txtâ€™\n",
            "\n",
            "elon_musk_speech.tx 100%[===================>]   9.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-04-26 09:47:52 (65.1 MB/s) - â€˜elon_musk_speech.txtâ€™ saved [10139/10139]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/alessiodevoto/deepers/main/data/elon_musk_speech.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOur4E7qYiyj"
      },
      "source": [
        "We could simply read the document in plain Python but we use [document loader](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) here just to keep the code homogeneous and have some nice features LangChain gives us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH76tcgTFxgZ"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('./elon_musk_speech.txt')\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "# it's just the content \n",
        "#documents[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XQkFrW_YVUu"
      },
      "source": [
        "As mentioned above, a whole document might be too large to fit the model's **context window**. So we split it into smaller chunks that will be embedded in a **vector space**. To do so, we use [text splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) together with embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtR0ElRbYExX"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# we can do this in a lot of different ways\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator='\\n')\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaRL_JneOZX1",
        "outputId": "d4cf19c6-6bc7-4a28-da01-36f58febf330"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Elon Musk, Magicians of the 21st Century, Caltech Commencement Address,\\njune, 2012', metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content=\"I'd like to thank you for leaving 'crazy person' out of your introduction. [Laughter].\", metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content=\"I was trying to think what's the most useful thing that I can say to be useful to you in the\", metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content='future. And I thought, perhaps tell the story of how I sort of came to be here. How did these', metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content='things happen? Maybe there are lessons there. I often find myself wondering, how did this\\nhappen.', metadata={'source': './elon_musk_speech.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "texts[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS4vI-BvZYKk"
      },
      "source": [
        "We use a [vector store](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html), which takes care of actually projecting our elements into the vector space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXNVfN2_YI0k",
        "outputId": "970528f5-e2b5-490e-f707-eb704c7e2e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "db = Chroma.from_documents(texts, OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YSDdeC8h7u9"
      },
      "source": [
        "Let's just make a quick similarity search. Given a query, we look for the embeddings (= chunks of the intial documents) that are the most similar to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXOFLRsicRCf",
        "outputId": "bc89b13e-7159-4d2c-e508-1dcefbd42197"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Elon Musk, Magicians of the 21st Century, Caltech Commencement Address,\\njune, 2012', metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content='So, I studied physics and business, because in order to do these things you need to know how', metadata={'source': './elon_musk_speech.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "docs = db.similarity_search(\"what did Elon Musk study?\", k=2)\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PaqkljAiPwf"
      },
      "source": [
        "Finally, we can combine the database with a language model in a Retrieval Chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lkzL8EYYMFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18403e5-ec78-4330-8896-90fdc966d01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:WARNING! model is not default parameter.\n",
            "                    model was transfered to model_kwargs.\n",
            "                    Please confirm that model is what you intended.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# create a chain with given llm \n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(model='text-davinci-002', temperature=0), \n",
        "    chain_type=\"stuff\",      # how should we treat the document\n",
        "    retriever=db.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NOnd2VDfbDgP",
        "outputId": "1f504a41-e357-4e38-fb0c-f72a875785fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nElon Musk studied physics and business.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "query = \"What did Elon Musk study?\"\n",
        "qa_chain.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPEc27DPjWdQ"
      },
      "source": [
        "Seems pretty accurate!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dRNp9kdwjKfo",
        "outputId": "cb4bad00-88f0-46d4-ae76-dd1946bbe8aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "query = \"What did Elon Musk say about DeepCamp?\"\n",
        "qa_chain.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI0iLrjNi0i4"
      },
      "source": [
        "What if we want to add some info on the fly?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTt-qJH-i0F-",
        "outputId": "26462280-1256-48e8-8e82-3a14ff2a7dee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['7fa257a0-e417-11ed-9b06-0242ac1c000c',\n",
              " '7fa25a16-e417-11ed-9b06-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "db.add_texts(['DeepCamp is an amazing event held in Milan for people who want to learn about deep learning.', 'I would love to go to Deepcamp, but the organizers forgot to invite me.'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZetkypWjPhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec381786-2a9a-4f0b-fcd9-cb7b3f418764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:WARNING! model is not default parameter.\n",
            "                    model was transfered to model_kwargs.\n",
            "                    Please confirm that model is what you intended.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# create a chain with given llm \n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(model='text-davinci-002', temperature=0), \n",
        "    chain_type=\"stuff\",      # how should we treat the document\n",
        "    retriever=db.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Na_f9LVxjbaP",
        "outputId": "fde01ce7-d13f-47f8-b8fb-bd1d296f4520"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' He said that he would love to go to DeepCamp, but the organizers forgot to invite him.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "query = \"What did he say about DeepCamp?\"\n",
        "qa_chain.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "q9I-yDpEjjvx",
        "outputId": "6094ef37-5705-4066-8b7a-6cb8879dd1e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nThe organizers forgot to invite the speaker to Deepcamp.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "query = \"did the organizers forget anything?\"\n",
        "qa_chain.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Exercise ðŸ”¥\n",
        "\n",
        "Make a Sequential chain composed of:\n",
        "\n",
        "- a chain that reads a document of recipes and provides the steps for a recipes as output.\n",
        "- a chain that takes the recipe as input the recipes and tells the user which ingredients they should buy.\n",
        "\n",
        "Follow the template.\n"
      ],
      "metadata": {
        "id": "s9A-7j3gfOps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/alessiodevoto/deepers/main/data/recipes.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvjzGq3stCSE",
        "outputId": "13c8d3e0-9e6a-4627-b956-8d5f420950a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-26 09:12:40--  https://raw.githubusercontent.com/alessiodevoto/deepers/main/data/recipes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4164 (4.1K) [text/plain]\n",
            "Saving to: â€˜recipes.txtâ€™\n",
            "\n",
            "\rrecipes.txt           0%[                    ]       0  --.-KB/s               \rrecipes.txt         100%[===================>]   4.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-04-26 09:12:40 (43.5 MB/s) - â€˜recipes.txtâ€™ saved [4164/4164]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load and have a look at the file\n",
        "2. Split it into chunks (Is there any smart way to split it ?)\n"
      ],
      "metadata": {
        "id": "o-YLcatDtbzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Peek solution ðŸ‘€\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# load doc\n",
        "loader = TextLoader('./recipes.txt')\n",
        "documents = loader.load()\n",
        "# split\n",
        "text_splitter = CharacterTextSplitter(chunk_size=600, chunk_overlap=0, separator='\\n\\n')\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL7rU6EFluA5",
        "outputId": "24500687-cc6c-44fd-afe5-4fc854c3b669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 647, which is longer than the specified 600\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 712, which is longer than the specified 600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='1. Spaghetti Bolognese: \\nThis classic Italian dish is a favorite of many. Start by heating a large pot on medium heat and adding a tablespoon of butter or olive oil. Add some minced garlic and diced onions and cook until softened. Add ground beef, breaking it up with a wooden spoon as it cooks. Once the beef is cooked, add a jar of tomato sauce, a can of diced tomatoes, a tablespoon of Italian seasoning, a teaspoon of sugar, and some freshly ground black pepper. Simmer for about 20 minutes and then add cooked spaghetti to the pot. Mix until combined and serve with grated Parmesan cheese.', metadata={'source': './recipes.txt'})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a database using OpenAIEmbeddings\n",
        "4. Make a quick similarity search with 'Tacos'"
      ],
      "metadata": {
        "id": "uWaw4K8CvV2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "KLpqcgXvvtux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Peek solution ðŸ‘€\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "db = Chroma.from_documents(texts, OpenAIEmbeddings())\n",
        "\n",
        "docs = db.similarity_search(\"Bolognese\", k=1)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnYyFCCzoCpi",
        "outputId": "e3c94847-e526-4a2c-908a-e6115729ba27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='1. Spaghetti Bolognese: \\nThis classic Italian dish is a favorite of many. Start by heating a large pot on medium heat and adding a tablespoon of butter or olive oil. Add some minced garlic and diced onions and cook until softened. Add ground beef, breaking it up with a wooden spoon as it cooks. Once the beef is cooked, add a jar of tomato sauce, a can of diced tomatoes, a tablespoon of Italian seasoning, a teaspoon of sugar, and some freshly ground black pepper. Simmer for about 20 minutes and then add cooked spaghetti to the pot. Mix until combined and serve with grated Parmesan cheese.', metadata={'source': './recipes.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Allocate the RetrievealQA chain"
      ],
      "metadata": {
        "id": "OkP8Y6QUvmF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "kXeQuRvgvsD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Peek solution ðŸ‘€\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# create a chain with given llm \n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm, \n",
        "    chain_type=\"stuff\",      \n",
        "    retriever=db.as_retriever(),\n",
        "    verbose=True\n",
        "    )\n"
      ],
      "metadata": {
        "id": "iT7wpr8Yo8gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Create a simple LLM chain with a prompt template. Given a recipe, the chain should tell us which ingredients to buy."
      ],
      "metadata": {
        "id": "4DL-rFLXvyfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "0FqzVTLKv8RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "template = \"\"\"Given the recipe for a dish, write a list of the ingredients I should buy.\n",
        "\n",
        "Recipe:\n",
        "{recipe}\n",
        "Here are the ingredients you should buy:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"recipe\"], template=template)\n",
        "ingredients_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "0Yzp_x2gpp2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Create and run a sequential chain with the prompt: `\"Can you tell me how to make spaghetti Bolognese?\"`"
      ],
      "metadata": {
        "id": "AniXwQ_Av_Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the overall chain where we run these two chains in sequence.\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[qa_chain, ingredients_chain], verbose=True)\n",
        "\n",
        "query = \"Can you tell me how to make spaghetti Bolognese?\"\n",
        "overall_chain.run('Bolognese')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "GvWaKRZIqrf1",
        "outputId": "0d82a9bd-ad8d-46a2-ca63-ac62462b6e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m\n",
            "\n",
            "Spaghetti Bolognese is a classic Italian dish that is a favorite of many. To make it, start by heating a large pot on medium heat and adding a tablespoon of butter or olive oil. Add some minced garlic and diced onions and cook until softened. Add ground beef, breaking it up with a wooden spoon as it cooks. Once the beef is cooked, add a jar of tomato sauce, a can of diced tomatoes, a tablespoon of Italian seasoning, a teaspoon of sugar, and some freshly ground black pepper. Simmer for about 20 minutes and then add cooked spaghetti to the pot. Mix until combined and serve with grated Parmesan cheese.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m\n",
            "\n",
            "-Butter or olive oil\n",
            "-Garlic\n",
            "-Onions\n",
            "-Ground beef\n",
            "-Tomato sauce\n",
            "-Diced tomatoes\n",
            "-Italian seasoning\n",
            "-Sugar\n",
            "-Pepper\n",
            "-Spaghetti\n",
            "-Parmesan cheese\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n-Butter or olive oil\\n-Garlic\\n-Onions\\n-Ground beef\\n-Tomato sauce\\n-Diced tomatoes\\n-Italian seasoning\\n-Sugar\\n-Pepper\\n-Spaghetti\\n-Parmesan cheese'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want to add a step to translate the recipe to italian? ðŸ¤” I'll leave that to you ðŸ˜‰"
      ],
      "metadata": {
        "id": "au46sahxwokH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}