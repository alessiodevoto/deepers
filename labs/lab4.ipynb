{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVYJg6pGEaS0"
      },
      "source": [
        "# Deepcamp: Codelab 4\n",
        "\n",
        "**In this tutorial we will cover**:\n",
        "\n",
        "- Large Language Models (ever heard of ChatGPT ðŸ‘€ ?)\n",
        "- LangChain: a tool to allow interaction between LLMs\n",
        "\n",
        "\n",
        "**Author**:\n",
        "- Alessio Devoto (alessio.devoto@uniroma1.it)\n",
        "\n",
        "\n",
        "**Duration**: 50 mins \n",
        "\n",
        "\n",
        "**Warning**: Make sure you have an OpenAI token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jIOwSLhQ0a05"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "bjhb8WWF9toT"
      },
      "outputs": [],
      "source": [
        "# usual general imports \n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYEswD8hEcaA"
      },
      "source": [
        "# LangChain\n",
        "\n",
        "Langchain makes interaction with Large Language Models easy and intuitive.\n",
        "\n",
        "The whole library is comprised of the following building blocks:\n",
        "\n",
        "- models: a model represents an llm or llm-related service offered by an API endpoint\n",
        "- prompts: a prompt is what we feed as input to the model\n",
        "- indexes: an index is a set of rules to split, embed and store documents so that language models can best interact with them\n",
        "- chains: a sequence of modular components (models, prompts, other chains ...)  combined in a particular way \n",
        "\n",
        "The library also offers other modules that we are not interested in today: Agents and Memory. Please refer to the [LangChain official](https://docs.langchain.com/docs/) guide for those.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Zl1r5eZKhz"
      },
      "source": [
        "## Models\n",
        "\n",
        "Let us start by exploring a simple Langchain Model.\n",
        "- Model allows us to interact with a number of LLMs providers with a uniform interface. \n",
        "- Check the list of supported LLMs [here](https://python.langchain.com/en/latest/modules/models/llms/integrations.html)\n",
        "\n",
        "We are going to use OpenAI's API, which exposes a lot of [versions](https://platform.openai.com/docs/models/gpt-4) of ChatGPT. \n",
        "\n",
        "To do so, you must have an OpenAI account and generate an [access key](https://platform.openai.com/account/api-keys).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "eIeaJRs9aMfo"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "3l3f67UF6zGj"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    model_name=\"text-davinci-002\",    # you can pick other models but this is the fastest one                     \n",
        "    temperature=0 # for OpenAi, see options here: https://platform.openai.com/docs/api-reference/completions/create\n",
        "    ) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59fR5lmC68sJ",
        "outputId": "2019a90b-06fa-4199-a1bb-1c46672e8401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "There is no one-size-fits-all answer to this question, as the best way to learn about deep learning and AI will vary depending on your level of expertise and experience. However, some suggestions for how to learn about deep learning and AI include attending conferences and workshops, reading books and articles on the topic, and taking online courses.\n"
          ]
        }
      ],
      "source": [
        "# let's ask something to the language model\n",
        "\n",
        "llm_result = llm(\"What should I do to learn about Deep Learning and AI ?\")\n",
        "print(llm_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffPcD8vADxrl"
      },
      "source": [
        "- llm chain vs llm-index related chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ1sm5xxa0y5"
      },
      "source": [
        "## Prompts\n",
        "\n",
        "A prompt is a nice way to format the models input before it is actually fed to the model. \n",
        "\n",
        "This can be useful in case we want to keep part of the prompt hidden from the user but still provide an optimal response.\n",
        "\n",
        "Let us ask a simple question to the LLM. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDxZA_9Fug2r"
      },
      "outputs": [],
      "source": [
        "question =  \"There are 16 balls. Half of the balls are golf balls. Half of the golf balls are blue balls. How many blue golf balls are there?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IVYRPlLucBt",
        "outputId": "0c3d527f-474c-4f64-830a-00ea6a307724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "There are 8 blue golf balls.\n"
          ]
        }
      ],
      "source": [
        "llm_results = llm(question)\n",
        "print(llm_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXghfkJnuzD9"
      },
      "source": [
        "Ok, looks like it got it wrong... ðŸ¤“ Can we provide a better question which can help the model give a [better answer](https://arxiv.org/pdf/2205.11916.pdf) ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znI6KaQhvJuP",
        "outputId": "4baa1c18-a113-4d0f-ee45-bfd500bd5ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Prompt: \n",
            "There are 16  balls. Half of the balls are golf balls. Half of the golf balls are blue balls. How many blue golf balls are there?\n",
            "Let's think about this step by step\n",
            "\n",
            "-----------\n",
            "LLM Output: There are 16 balls\n",
            "There are 8 golf balls\n",
            "There are 4 blue golf balls\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# question will be replaced by user's prompt\n",
        "template = \"\"\"\n",
        "{question}\n",
        "Let's think about this step by step\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(riddle=question)\n",
        "\n",
        "print (f\"Final Prompt: {final_prompt}\")\n",
        "\n",
        "print (f\"LLM Output: {llm(final_prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMiFwuIXA2vU"
      },
      "source": [
        "## Chains\n",
        "\n",
        "Chains allow us to chain ðŸ˜ƒ modules and prompts in order to attain a task specific goal. \n",
        "There are several kinds of chains. We can identify two main categories:\n",
        "\n",
        "- LLM Chains: interaction of Model, Prompt and OutputParsers\n",
        "- Index-Related Chains: LLM chains to deal with Documents and External \"Memory\"\n",
        "\n",
        "In addition, we have a number of chains that allow you to interact in different fashions with all the modules in the library or even your own local machine processes (See later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFu8x4UDG8Y0"
      },
      "source": [
        "### LLM Chain\n",
        "\n",
        "A simple LLM Chain accepts two main input arguments:\n",
        "1. The LLM `Model` to be used\n",
        "2. The `Prompt` template to be used (if any)\n",
        "\n",
        "You can also add an output parser at the end of the chain to edit the chain's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ4gyibeFKow"
      },
      "outputs": [],
      "source": [
        "from langchain import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxzzbHL6HPr7"
      },
      "outputs": [],
      "source": [
        "# Exercise: Create an LLMChain with a prompt template that takes three input arguments and run with differnt inputs. \n",
        "# Follow this template\n",
        "\n",
        "template = \"\"\"Write a {adjective} poem about {subject} in {language}.\"\"\"\n",
        "prompt = \n",
        "llm_chain = LLMChain(\n",
        "    prompt = ..., \n",
        "    llm = ...,\n",
        "    verbose=True)\n",
        "\n",
        "results = llm_chain.run(\n",
        "    adjective=, \n",
        "    subject=,\n",
        "    language=\n",
        "    )\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaCd2ffuBfnB",
        "outputId": "679151d1-7214-4b22-e91d-099fa9a27762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a sad poem about ducks in slang english.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Ducks is some real sad birds\n",
            "They just waddle around all day\n",
            "And every time they quack\n",
            "It just sound like they're sayin' \"fuck\"\n",
            "\n",
            "Life ain't easy for no ducks\n",
            "They got webbed feet and feathers\n",
            "And they always look so cold\n",
            "They just waddle around in the water\n",
            "And every time they try to fly\n",
            "They just end up fallin' down\n",
            "\n",
            "Ducks is some real sad birds\n",
            "And I can't help but feel bad\n",
            "For everything they go through\n",
            "They just try to live their lives\n",
            "The best way they know how\n",
            "And I hope one day they'll find\n",
            "A place where they can be happy\n"
          ]
        }
      ],
      "source": [
        "# Solution\n",
        "template = \"\"\"Write a {adjective} poem about {subject} in {language}.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\", \"language\"])\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt, \n",
        "    llm=llm, \n",
        "    verbose=True)\n",
        "\n",
        "results = llm_chain.run(adjective=\"sad\", subject=\"ducks\", language=\"slang english\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLX8q7wbIKrg",
        "outputId": "0982e0ad-b732-47b5-9d18-e3eac2fcb058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a sad poem about ducks in italian.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "I piccoli anatroccoli\n",
            "\n",
            "nascono nel fango\n",
            "\n",
            "e si nutrono di insetti\n",
            "\n",
            "fino a quando non sono pronti\n",
            "\n",
            "per volare via\n",
            "\n",
            "lontano dal loro nido.\n",
            "\n",
            "Ma prima di allora\n",
            "\n",
            "i cacciatori vengono\n",
            "\n",
            "e uccidono i piccoli\n",
            "\n",
            "anatroccoli\n",
            "\n",
            "per farne il prezioso\n",
            "\n",
            "piumaggio.\n",
            "\n",
            "Ogni anno\n",
            "\n",
            "i piccoli anatroccoli\n",
            "\n",
            "muoiono\n",
            "\n",
            "senza poter mai\n",
            "\n",
            "volare via.\n"
          ]
        }
      ],
      "source": [
        "# Just cause it's fun\n",
        "\n",
        "results = llm_chain.run(adjective=\"sad\", subject=\"ducks\", language=\"italian\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise ðŸ‹: Bash Chain\n",
        "\n",
        "Allocate an LLM Bash chain which chains the LM with a bash process on your local machine. Then ask the llm to create a bash script which echoes `DeepCamp is so cool!` inside the shell.\n",
        "\n",
        "\n",
        "Follow the steps in the template."
      ],
      "metadata": {
        "id": "G_slrPMFI04t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMBashChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# 1. Create the language model\n",
        "llm = \n",
        "\n",
        "# 2. Create the chain\n",
        "bash_chain = LLMBashChain(llm=)\n",
        "\n",
        "# 3. Create prompt and run the chain\n",
        "# You should be very clear about what you want the llm to do, as we are using an old version of GPT and it could misunderstand you \n",
        "\n"
      ],
      "metadata": {
        "id": "Pu-1d9DeK5Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Peek Solution ðŸ‘€\n",
        "\n",
        "from langchain.chains import LLMBashChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-002\", temperature=0)\n",
        "\n",
        "text = \"Please write a bash script that prints 'DeepCamp is so cool!' to the console.\"\n",
        "\n",
        "bash_chain = LLMBashChain(llm=llm, verbose=True)\n",
        "\n",
        "bash_chain.run(text) # this is reallt being executed in a bash process on your local machine! "
      ],
      "metadata": {
        "id": "dgRmR2goImyL",
        "outputId": "cfa08a9b-e5e8-442e-e5cb-fe2bb29ee104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
            "Please write a bash script that prints 'DeepCamp is so cool!' to the console.\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "```bash\n",
            "echo \"DeepCamp is so cool!\"\n",
            "```\u001b[0m['```bash', 'echo \"DeepCamp is so cool!\"', '```']\n",
            "\n",
            "Answer: \u001b[33;1m\u001b[1;3mDeepCamp is so cool!\n",
            "\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DeepCamp is so cool!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now ask the chain to create a directory named `labs_deepcamp`, run it, and check if the dir was actually created."
      ],
      "metadata": {
        "id": "p29qngHOL-tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "s5mUXC6oMEk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Peek Solution ðŸ‘€\n",
        "\n",
        "text = \"Please write a bash script that creates a directory named 'labs_deepers'.\"\n",
        "\n",
        "bash_chain = LLMBashChain(llm=llm, verbose=True)\n",
        "\n",
        "bash_chain.run(text)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xsJai7oWKJtv",
        "outputId": "658d29e7-8343-4728-e265-ece539d4e2a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
            "Please write a bash script that creates a directory named 'labs_deepers'.\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "```bash\n",
            "mkdir labs_deepers\n",
            "```\u001b[0m['```bash', 'mkdir labs_deepers', '```']\n",
            "\n",
            "Answer: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj1BNF21I0ms"
      },
      "source": [
        "## Indexes \n",
        "\n",
        "The concept of Index in LangChain is quite broad. Indexes refer to ways to structure documents so that LLMs can best interact with them.\n",
        "\n",
        "A naive way of combining LLMs and documents would be injecting the content of a document (assuming it is a text document) into the LLM as a simple prompt. This might work for small docs, as the may possibly fit the LLM [context](https://www.theatlantic.com/technology/archive/2023/03/gpt-4-has-memory-context-window/673426/). When we deal with larger or multiple documents though, this is not feasible. \n",
        "\n",
        "We have four classes composing the Index module:\n",
        "\n",
        "- **Document Loaders**: responsible for loading documents from various sources.\n",
        "\n",
        "- **Text Splitters**: responsible for splitting text into smaller chunks.\n",
        "\n",
        "- **VectorStores**: Databases of stored documents\n",
        "\n",
        "- **Retrievers**: nterface for fetching relevant documents to combine with language models.\n",
        "\n",
        "\n",
        "As a first step, let us download a txt document containing a famouse speech by Elon Musk."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/alessiodevoto/deepers/main/elon_musk_speech.txt"
      ],
      "metadata": {
        "id": "3SM0yaEON3Qr",
        "outputId": "22524352-9ed1-4ffd-f8f6-336b891989a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-19 16:14:01--  https://raw.githubusercontent.com/alessiodevoto/deepers/main/elon_musk_speech.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10139 (9.9K) [text/plain]\n",
            "Saving to: â€˜elon_musk_speech.txtâ€™\n",
            "\n",
            "\relon_musk_speech.tx   0%[                    ]       0  --.-KB/s               \relon_musk_speech.tx 100%[===================>]   9.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-04-19 16:14:01 (75.0 MB/s) - â€˜elon_musk_speech.txtâ€™ saved [10139/10139]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could simply read the document in plain Python but we use [document loader](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) here just to keep the code homogeneous and have some nice features LangChain gives us."
      ],
      "metadata": {
        "id": "dOur4E7qYiyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('./elon_musk_speech.txt')\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "# it's just the content \n",
        "#documents[:10]"
      ],
      "metadata": {
        "id": "mH76tcgTFxgZ"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned above, a whole document might be too large to fit the model's context window. So we split it into smaller chunks that will be embedded in a vector space. To do so, we use [text splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) together with embeddings."
      ],
      "metadata": {
        "id": "2XQkFrW_YVUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# we can do this in a lot of different ways\n",
        "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10, separator='\\n')\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "FtR0ElRbYExX"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:5]"
      ],
      "metadata": {
        "id": "LaRL_JneOZX1",
        "outputId": "6525f863-8256-43d7-8d27-ceeaa7bf77de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"Elon Musk's Speech:\", metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content=\"I'd like to thank you for leaving 'crazy person' out of your introduction. [Laughter].\", metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content=\"I was trying to think what's the most useful thing that I can say to be useful to you in the\", metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content='future. And I thought, perhaps tell the story of how I sort of came to be here. How did these', metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content='things happen? Maybe there are lessons there. I often find myself wondering, how did this\\nhappen.', metadata={'source': './elon_musk_speech.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a [vector store](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html), which takes care of actually projecting our elements into the vector space."
      ],
      "metadata": {
        "id": "CS4vI-BvZYKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "db = Chroma.from_documents(texts, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "FXNVfN2_YI0k",
        "outputId": "8149a99c-abee-4abc-c94f-d5362d08359b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's just make a quick similarity search. Given a query, we look for the embeddings (= chunks of the intial documents) that are the most similar to the query."
      ],
      "metadata": {
        "id": "1YSDdeC8h7u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = db.similarity_search(\"what did Elon Musk study?\")\n",
        "docs"
      ],
      "metadata": {
        "id": "mXOFLRsicRCf",
        "outputId": "aeb38391-aee2-4a56-cc10-2487f5bbce27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"Elon Musk's Speech:\", metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content='So, I studied physics and business, because in order to do these things you need to know how', metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content=\"started SpaceX, initially, I thought that well, there's no way one could start a rocket company.\", metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content=\"sort of the thing that we're going to try to achieve with SpaceX.\", metadata={'source': './elon_musk_speech.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can combine the database with a language model in a Retrieval Chain."
      ],
      "metadata": {
        "id": "1PaqkljAiPwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# create a chain with given llm \n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm, \n",
        "    chain_type=\"stuff\",      # how should we treat the document\n",
        "    retriever=db.as_retriever())"
      ],
      "metadata": {
        "id": "9lkzL8EYYMFm"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did Elon Musk study?\"\n",
        "qa_chain.run(query)"
      ],
      "metadata": {
        "id": "NOnd2VDfbDgP",
        "outputId": "0b664624-6985-4e46-a78a-03225d39a585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nElon Musk studied physics and business.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems pretty accurate!"
      ],
      "metadata": {
        "id": "bPEc27DPjWdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did Elon Musk say about DeepCamp?\"\n",
        "qa_chain.run(query)"
      ],
      "metadata": {
        "id": "dRNp9kdwjKfo",
        "outputId": "fa29b9b4-ed2b-49f0-cfed-7d5df5cb56ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" DeepCamp is a sort of the thing that we're going to try to achieve with SpaceX.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we want to add some info on the fly?"
      ],
      "metadata": {
        "id": "QI0iLrjNi0i4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db.add_texts(['DeepCamp is an amazing event held in Milan for people who want to learn about deep learning.', 'I would love to go to Deepcamp, but the organizers forgot to invite me.'])"
      ],
      "metadata": {
        "id": "UTt-qJH-i0F-",
        "outputId": "7200925b-1571-4a15-f40e-221828ecac46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3e924a50-deda-11ed-a2a6-0242ac1c000c',\n",
              " '3e924c6c-deda-11ed-a2a6-0242ac1c000c']"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# create a chain with given llm \n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(temperature=0), \n",
        "    chain_type=\"stuff\",      # how should we treat the document\n",
        "    retriever=db.as_retriever())"
      ],
      "metadata": {
        "id": "_ZetkypWjPhZ"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What did Elon Musk say about deepcamp?\"\n",
        "qa_chain.run(query)"
      ],
      "metadata": {
        "id": "Na_f9LVxjbaP",
        "outputId": "7be71aab-d7fc-4dba-c97f-b71b741eecd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Elon Musk said that he would love to go to Deepcamp, but the organizers forgot to invite him.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"did the organizers forget anything?\"\n",
        "qa_chain.run(query)"
      ],
      "metadata": {
        "id": "q9I-yDpEjjvx",
        "outputId": "a262ade6-ec22-4926-b5bf-c3ba44052366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Yes, it appears that the organizers forgot to invite you.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = db.similarity_search(\"What did he say about deepcamp?\")\n",
        "docs"
      ],
      "metadata": {
        "id": "6dGu420vmY3q",
        "outputId": "a46ea49c-0462-42d5-d437-92601be730a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='DeepCamp is an amazing event held in Milan for people who want to learn about deep learning.', metadata={}),\n",
              " Document(page_content='I would love to go to Deepcamp, but the organizers forgot to invite me.', metadata={}),\n",
              " Document(page_content=\"Elon Musk, Magicians of the 21st Century, Caltech Commencement Address,\\njune, 2012\\n\\nI'd like to thank you for leaving 'crazy person' out of your introduction. [Laughter].\", metadata={'source': './elon_musk_speech.txt'}),\n",
              " Document(page_content='then developed the Dragon spacecraft, which recently docked to the space station and\\nreturned to earth.', metadata={'source': './elon_musk_speech.txt'})]"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}