{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVYJg6pGEaS0"
      },
      "source": [
        "# Deepcamp: Codelab 4\n",
        "\n",
        "**In this tutorial we will cover**:\n",
        "\n",
        "- Large Language Models (ever heard of ChatGPT ðŸ‘€ ?)\n",
        "- LangChain: a tool to allow interaction between LLMs\n",
        "\n",
        "\n",
        "**Author**:\n",
        "- Alessio Devoto (alessio.devoto@uniroma1.it)\n",
        "\n",
        "\n",
        "**Duration**: 50 mins \n",
        "\n",
        "\n",
        "**Warning**: Make sure you have an OpenAI token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIOwSLhQ0a05"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain\n",
        "!pip install openai\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjhb8WWF9toT"
      },
      "outputs": [],
      "source": [
        "# usual general imports \n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYEswD8hEcaA"
      },
      "source": [
        "# LangChain\n",
        "\n",
        "Langchain makes interaction with Large Language Models easy and intuitive.\n",
        "\n",
        "The whole library is comprised of the following building blocks:\n",
        "\n",
        "- models: a model represents an llm or llm-related service offered by an API endpoint\n",
        "- prompts: a prompt is what we feed as input to the model\n",
        "- indexes: an index is a set of rules to split, embed and store documents so that language models can best interact with them\n",
        "- chains: a sequence of modular components (models, prompts, other chains ...)  combined in a particular way \n",
        "\n",
        "The library also offers other modules that we are not interested in today: Agents and Memory. Please refer to the [LangChain official](https://docs.langchain.com/docs/) guide for those.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Zl1r5eZKhz"
      },
      "source": [
        "## Models\n",
        "\n",
        "Let us start by exploring a simple Langchain Model.\n",
        "- Model allows us to interact with a number of LLMs providers with a uniform interface. \n",
        "- Check the list of supported LLMs [here](https://python.langchain.com/en/latest/modules/models/llms/integrations.html)\n",
        "\n",
        "We are going to use OpenAI's API, which exposes a lot of [versions](https://platform.openai.com/docs/models/gpt-4) of ChatGPT. \n",
        "\n",
        "To do so, you must have an OpenAI account and generate an [access key](https://platform.openai.com/account/api-keys).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIeaJRs9aMfo"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l3f67UF6zGj"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(\n",
        "    model_name=\"text-davinci-002\",    # you can pick other models but this is the fastest one                     \n",
        "    temperature=0 # for OpenAi, see options here: https://platform.openai.com/docs/api-reference/completions/create\n",
        "    ) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59fR5lmC68sJ",
        "outputId": "2019a90b-06fa-4199-a1bb-1c46672e8401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "There is no one-size-fits-all answer to this question, as the best way to learn about deep learning and AI will vary depending on your level of expertise and experience. However, some suggestions for how to learn about deep learning and AI include attending conferences and workshops, reading books and articles on the topic, and taking online courses.\n"
          ]
        }
      ],
      "source": [
        "# let's ask something to the language model\n",
        "\n",
        "llm_result = llm(\"What should I do to learn about Deep Learning and AI ?\")\n",
        "print(llm_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffPcD8vADxrl"
      },
      "source": [
        "- llm chain vs llm-index related chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ1sm5xxa0y5"
      },
      "source": [
        "## Prompts\n",
        "\n",
        "A prompt is a nice way to format the models input before it is actually fed to the model. \n",
        "\n",
        "This can be useful in case we want to keep part of the prompt hidden from the user but still provide an optimal response.\n",
        "\n",
        "Let us ask a simple question to the LLM. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDxZA_9Fug2r"
      },
      "outputs": [],
      "source": [
        "question =  \"There are 16 balls. Half of the balls are golf balls. Half of the golf balls are blue balls. How many blue golf balls are there?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IVYRPlLucBt",
        "outputId": "0c3d527f-474c-4f64-830a-00ea6a307724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "There are 8 blue golf balls.\n"
          ]
        }
      ],
      "source": [
        "llm_results = llm(question)\n",
        "print(llm_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXghfkJnuzD9"
      },
      "source": [
        "Ok, looks like it got it wrong... ðŸ¤“ Can we provide a better question which can help the model give a [better answer](https://arxiv.org/pdf/2205.11916.pdf) ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znI6KaQhvJuP",
        "outputId": "4baa1c18-a113-4d0f-ee45-bfd500bd5ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Prompt: \n",
            "There are 16  balls. Half of the balls are golf balls. Half of the golf balls are blue balls. How many blue golf balls are there?\n",
            "Let's think about this step by step\n",
            "\n",
            "-----------\n",
            "LLM Output: There are 16 balls\n",
            "There are 8 golf balls\n",
            "There are 4 blue golf balls\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# question will be replaced by user's prompt\n",
        "template = \"\"\"\n",
        "{question}\n",
        "Let's think about this step by step\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(riddle=question)\n",
        "\n",
        "print (f\"Final Prompt: {final_prompt}\")\n",
        "\n",
        "print (f\"LLM Output: {llm(final_prompt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMiFwuIXA2vU"
      },
      "source": [
        "## Chains\n",
        "\n",
        "Chains allow us to chain ðŸ˜ƒ modules and prompts in order to attain a task specific goal. \n",
        "There are several kinds of chains. We can identify two main categories:\n",
        "\n",
        "- LLM Chains: interaction of Model, Prompt and OutputParsers\n",
        "- Index-Related Chains: LLM chains to deal with Documents and External \"Memory\"\n",
        "\n",
        "In addition, we have a number of chains that allow you to interact in different fashions with all the modules in the library or even your own local machine processes (See later)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFu8x4UDG8Y0"
      },
      "source": [
        "### LLM Chain\n",
        "\n",
        "A simple LLM Chain accepts two main input arguments:\n",
        "1. The LLM `Model` to be used\n",
        "2. The `Prompt` template to be used (if any)\n",
        "\n",
        "You can also add an output parser at the end of the chain to edit the chain's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ4gyibeFKow"
      },
      "outputs": [],
      "source": [
        "from langchain import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxzzbHL6HPr7"
      },
      "outputs": [],
      "source": [
        "# Exercise: Create an LLMChain with a prompt template that takes three input arguments and run with differnt inputs. \n",
        "# Follow this template\n",
        "\n",
        "template = \"\"\"Write a {adjective} poem about {subject} in {language}.\"\"\"\n",
        "prompt = \n",
        "llm_chain = LLMChain(\n",
        "    prompt = ..., \n",
        "    llm = ...,\n",
        "    verbose=True)\n",
        "\n",
        "results = llm_chain.run(\n",
        "    adjective=, \n",
        "    subject=,\n",
        "    language=\n",
        "    )\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaCd2ffuBfnB",
        "outputId": "679151d1-7214-4b22-e91d-099fa9a27762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a sad poem about ducks in slang english.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Ducks is some real sad birds\n",
            "They just waddle around all day\n",
            "And every time they quack\n",
            "It just sound like they're sayin' \"fuck\"\n",
            "\n",
            "Life ain't easy for no ducks\n",
            "They got webbed feet and feathers\n",
            "And they always look so cold\n",
            "They just waddle around in the water\n",
            "And every time they try to fly\n",
            "They just end up fallin' down\n",
            "\n",
            "Ducks is some real sad birds\n",
            "And I can't help but feel bad\n",
            "For everything they go through\n",
            "They just try to live their lives\n",
            "The best way they know how\n",
            "And I hope one day they'll find\n",
            "A place where they can be happy\n"
          ]
        }
      ],
      "source": [
        "# Solution\n",
        "template = \"\"\"Write a {adjective} poem about {subject} in {language}.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"adjective\", \"subject\", \"language\"])\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt, \n",
        "    llm=llm, \n",
        "    verbose=True)\n",
        "\n",
        "results = llm_chain.run(adjective=\"sad\", subject=\"ducks\", language=\"slang english\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLX8q7wbIKrg",
        "outputId": "0982e0ad-b732-47b5-9d18-e3eac2fcb058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWrite a sad poem about ducks in italian.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "I piccoli anatroccoli\n",
            "\n",
            "nascono nel fango\n",
            "\n",
            "e si nutrono di insetti\n",
            "\n",
            "fino a quando non sono pronti\n",
            "\n",
            "per volare via\n",
            "\n",
            "lontano dal loro nido.\n",
            "\n",
            "Ma prima di allora\n",
            "\n",
            "i cacciatori vengono\n",
            "\n",
            "e uccidono i piccoli\n",
            "\n",
            "anatroccoli\n",
            "\n",
            "per farne il prezioso\n",
            "\n",
            "piumaggio.\n",
            "\n",
            "Ogni anno\n",
            "\n",
            "i piccoli anatroccoli\n",
            "\n",
            "muoiono\n",
            "\n",
            "senza poter mai\n",
            "\n",
            "volare via.\n"
          ]
        }
      ],
      "source": [
        "# Just cause it's fun\n",
        "\n",
        "results = llm_chain.run(adjective=\"sad\", subject=\"ducks\", language=\"italian\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise ðŸ‹: Bash Chain\n",
        "\n",
        "Allocate an LLM Bash chain which chains the LM with a bash process on your local machine. Then ask the llm to create a bash script which echoes `DeepCamp is so cool!` inside the shell.\n",
        "\n",
        "\n",
        "Follow the steps in the template."
      ],
      "metadata": {
        "id": "G_slrPMFI04t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMBashChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# 1. Create the language model\n",
        "llm = \n",
        "\n",
        "# 2. Create the chain\n",
        "bash_chain = LLMBashChain(llm=)\n",
        "\n",
        "# 3. Create prompt and run the chain\n",
        "# You should be very clear about what you want the llm to do, as we are using an old version of GPT and it could misunderstand you \n",
        "\n"
      ],
      "metadata": {
        "id": "Pu-1d9DeK5Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Peek Solution ðŸ‘€\n",
        "\n",
        "from langchain.chains import LLMBashChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-002\", temperature=0)\n",
        "\n",
        "text = \"Please write a bash script that prints 'DeepCamp is so cool!' to the console.\"\n",
        "\n",
        "bash_chain = LLMBashChain(llm=llm, verbose=True)\n",
        "\n",
        "bash_chain.run(text) # this is reallt being executed in a bash process on your local machine! "
      ],
      "metadata": {
        "id": "dgRmR2goImyL",
        "outputId": "cfa08a9b-e5e8-442e-e5cb-fe2bb29ee104",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
            "Please write a bash script that prints 'DeepCamp is so cool!' to the console.\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "```bash\n",
            "echo \"DeepCamp is so cool!\"\n",
            "```\u001b[0m['```bash', 'echo \"DeepCamp is so cool!\"', '```']\n",
            "\n",
            "Answer: \u001b[33;1m\u001b[1;3mDeepCamp is so cool!\n",
            "\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DeepCamp is so cool!\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now ask the chain to create a directory named `labs_deepcamp`."
      ],
      "metadata": {
        "id": "p29qngHOL-tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "s5mUXC6oMEk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Peek Solution ðŸ‘€\n",
        "\n",
        "text = \"Please write a bash script that creates a directory named 'labs_deepers'.\"\n",
        "\n",
        "bash_chain = LLMBashChain(llm=llm, verbose=True)\n",
        "\n",
        "bash_chain.run(text)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xsJai7oWKJtv",
        "outputId": "658d29e7-8343-4728-e265-ece539d4e2a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
            "Please write a bash script that creates a directory named 'labs_deepers'.\u001b[32;1m\u001b[1;3m\n",
            "\n",
            "```bash\n",
            "mkdir labs_deepers\n",
            "```\u001b[0m['```bash', 'mkdir labs_deepers', '```']\n",
            "\n",
            "Answer: \u001b[33;1m\u001b[1;3m\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj1BNF21I0ms"
      },
      "source": [
        "## Indexes \n",
        "\n",
        "The concept of Index in LangChain is quite broad. Indexes refer to ways to structure documents so that LLMs can best interact with them.\n",
        "\n",
        "A naive way of combining LLMs and documents would be injecting the content of a document (assuming it is a text document) into the LLM as a simple prompt. This might work for small docs, as the may possibly fit the LLM [context](https://www.theatlantic.com/technology/archive/2023/03/gpt-4-has-memory-context-window/673426/). When we deal with larger or multiple documents though, this is not feasible. \n",
        "\n",
        "We have four classes composing the Index module:\n",
        "\n",
        "- **Document Loaders**: responsible for loading documents from various sources.\n",
        "\n",
        "- **Text Splitters**: responsible for splitting text into smaller chunks.\n",
        "\n",
        "- **VectorStores**: Databases of stored documents\n",
        "\n",
        "- **Retrievers**: nterface for fetching relevant documents to combine with language models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[document loader](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)"
      ],
      "metadata": {
        "id": "dOur4E7qYiyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('../state_of_the_union.txt', encoding='utf8')\n",
        "\n",
        "documents = loader.load()\n",
        "\n"
      ],
      "metadata": {
        "id": "mH76tcgTFxgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[text splitters](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html)"
      ],
      "metadata": {
        "id": "2XQkFrW_YVUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "FtR0ElRbYExX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B65BZCPdYeP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "I74fcFLIYDjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[vector stores](https://python.langchain.com/en/latest/modules/indexes/vectorstores.html)"
      ],
      "metadata": {
        "id": "CS4vI-BvZYKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "db = Chroma.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "FXNVfN2_YI0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)"
      ],
      "metadata": {
        "id": "9lkzL8EYYMFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_creator = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=Chroma, \n",
        "    embedding=OpenAIEmbeddings(),\n",
        "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        ")"
      ],
      "metadata": {
        "id": "alwBVtw5YOIr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}