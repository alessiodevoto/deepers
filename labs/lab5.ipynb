{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+HjZYUfpM28cydCjU5Ik/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deepcamp: Codelab 5\n",
        "\n",
        "**In this tutorial we will cover**:\n",
        "\n",
        "- Deployment of apps based on ML or DL models\n",
        "\n",
        "\n",
        "**Author**:\n",
        "- Alessio Devoto (alessio.devoto@uniroma1.it)\n",
        "\n",
        "\n",
        "**Duration**: 50 mins \n",
        "\n"
      ],
      "metadata": {
        "id": "z8seNS9m74E7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Basic Gradio intro with a simple example (no ml function) + exercise\n",
        "\n",
        "2. Set up an app that does speech to text\n",
        "\n",
        "\n",
        "3. Set up an app that downloads a pretrained resnet and performs image recognition "
      ],
      "metadata": {
        "id": "y2axuPdY8dEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment of Machine Learning models \n",
        "\n",
        "Let's say you designed and trained a very cool and effective deep learning model. \n",
        "\n",
        "The model is now ready to be tested on a different data and users. What will you do ? \n",
        "\n",
        "- You could get in touch with someone who takes care of designing the infrastructure for your system ‚û° expensive and time consuming\n",
        "- You can use a framework for ML apps deployment and start it on any server (of course, you must own a server)\n",
        "\n",
        "---\n",
        "\n",
        "There are a few libraries we can use for this purpose: [Gradio](https://gradio.app/quickstart/), [StreamLit](https://streamlit.io/) and even come with UI features. If you want a lower level control (at the cose of more complexity), you can use [FastApi](https://fastapi.tiangolo.com/) or [Flask](https://flask.palletsprojects.com/en/2.3.x/).\n",
        "\n",
        "Today, we are going to use Gradio, a new library developed by Huggingface for showcasing their own models."
      ],
      "metadata": {
        "id": "tmixIcfvEMNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install necessary libraries as usual"
      ],
      "metadata": {
        "id": "xr2Vuosf_xl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FUEr_HyP7us7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U openai-whisper\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Gradio basics\n",
        "\n",
        "Gradio lets you create intuitive User Interfaces in plain Python, by simply listing panels, buttons, text areas etc...\n",
        "\n",
        "All [components](https://gradio.app/docs/#components) must be initialized inside a gradio.Block, which represents an area of the interface.\n",
        "\n",
        "Let's start with a simple example taken from [here](https://gradio.app/docs/#components).\n",
        "\n"
      ],
      "metadata": {
        "id": "WWmsmzZF_3W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:   # main gradio block, always necessary\n",
        "\n",
        "    # from here we can populate the UI as we wish\n",
        "    gr.Markdown(\"DeepCamp: the best place in the **world**\")\n",
        "    \n",
        "    with gr.Tab(label=\"Flip Text\"):\n",
        "        text_input = gr.Textbox()\n",
        "        text_output = gr.Textbox()\n",
        "        text_button = gr.Button(\"Flip\")\n",
        "   \n",
        "    with gr.Tab(label=\"Flip Image\"):\n",
        "        with gr.Row():\n",
        "            image_input = gr.Image()\n",
        "            image_output = gr.Image()\n",
        "        image_button = gr.Button(\"Flip\")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "k71aoJ5lIOLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, nothing happens if we click the buttons, as there is no function implemented yet.\n",
        "\n",
        "Let's add simple functions which manipulate the input data and return an output."
      ],
      "metadata": {
        "id": "lqihcJ9AOTQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# a function that flips a string\n",
        "def flip_text(x):\n",
        "    return x[::-1]\n",
        "\n",
        "# a function that flips an image\n",
        "def flip_image(x):\n",
        "    return np.fliplr(x)\n",
        "\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:   # main gradio block, always necessary\n",
        "\n",
        "    # from here we can populate the UI as we wish\n",
        "    gr.Markdown(\"DeepCamp: the best place in the **world**\")\n",
        "    \n",
        "    with gr.Tab(label=\"Flip Text\"):\n",
        "        text_input = gr.Textbox()\n",
        "        text_output = gr.Textbox()\n",
        "        text_button = gr.Button(\"Flip\")\n",
        "   \n",
        "    with gr.Tab(label=\"Flip Image\"):\n",
        "        with gr.Row():\n",
        "            image_input = gr.Image()\n",
        "            image_output = gr.Image()\n",
        "        image_button = gr.Button(\"Flip\")\n",
        "\n",
        "    # assign corresponding function to each button\n",
        "    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n",
        "\n",
        "    # notice that the image is passed to the function as a numpy array\n",
        "    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n",
        "\n",
        "demo.launch() # you can control the ports and the number of concurrent threads here\n"
      ],
      "metadata": {
        "id": "6bFZuhnkII7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise üèã: write a basic gradio demo\n",
        "\n",
        "Write a gradio demo that given an image blurs it based on the input value provided by the user.\n",
        "\n",
        "For the blur, use the provided function \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "gaussian_filter(image, sigma)\n",
        "```\n",
        "\n",
        "where `image` is the image and `sigma` is the amount of blur.\n",
        "\n",
        "Allow the user to select the amount of blur in the range (0,5) via a gradio Slider (see [here](https://gradio.app/docs/#slider-header))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SgGIM-30PokF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "# your code here"
      ],
      "metadata": {
        "id": "gAb5jmmTNRY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Peek solution üëÄ\n",
        "\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "\n",
        "def blur(image, amount):\n",
        "  return gaussian_filter(image, sigma=amount)\n",
        "\n",
        "with gr.Blocks() as demo:   # main gradio block, always necessary\n",
        "\n",
        "  # from here we can populate the UI as we wish\n",
        "  gr.Markdown(\"DeepCamp: the best place in the **world**\")\n",
        "\n",
        "  blur_amount = gr.Slider(minimum=0, maximum=5)\n",
        "  with gr.Row():\n",
        "      image_input = gr.Image()\n",
        "      image_output = gr.Image()\n",
        "  blur_btn = gr.Button('Blur')\n",
        "\n",
        "  # note how we pass the params\n",
        "  blur_btn.click(blur, inputs=[image_input, blur_amount], outputs=image_output)\n",
        "\n",
        "\n",
        "demo.launch()\n",
        "    "
      ],
      "metadata": {
        "id": "9tQL6n6LK-rX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Advanced Gradio Demo\n",
        "\n",
        "This was an easy example, can we take another step and make things more interesting? \n",
        "\n",
        "\n",
        "**Current scenario:** we developed a speech to text model which is able to trascribe human speech, and we want to make this service available to all our colleagues.\n"
      ],
      "metadata": {
        "id": "0sZd_8qPvCpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Whisper\n",
        "Meet OpenAI's Whisper!\n",
        "\n",
        "![whisper](https://openaicom.imgix.net/d9c13138-366f-49d3-b8bd-cb3f5a973a5b/asr-summary-of-model-architecture-desktop.svg?fm=auto&auto=compress,format&fit=min&w=1919&h=1551)\n",
        "\n",
        "\n",
        "[Whisper](https://arxiv.org/pdf/2212.04356.pdf) is an automatic speech recognition (ASR) system trained on *680.000 hours of multilingual and multitask* supervised data collected from the web ü§Ø. \n",
        "\n",
        "The Whisper architecture is a simple end-to-end approach, implemented as an encoder-decoder Transformer. As you may know, Encoder and Decoder blocks are neural networks based on the attention mechanism. \n",
        "\n",
        "As you can see on the [official repository](https://github.com/openai/whisper) they trained the model in different \"sizes\": \n",
        "\n",
        "```\n",
        "tiny, small, base, medium, large, large-v2"
      ],
      "metadata": {
        "id": "GN13saDMDXtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchaudio.utils import download_asset\n",
        "import IPython\n",
        "\n",
        "speech_file = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")\n",
        "IPython.display.Audio(speech_file)"
      ],
      "metadata": {
        "id": "uqnVznin_pcy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /root/.cache/torch/hub/torchaudio/tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav /content/"
      ],
      "metadata": {
        "id": "1bD0hCE7F3zc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"tiny\") # we load the tiny version\n",
        "result = model.transcribe(\n",
        "    speech_file, \n",
        "    language='en', \n",
        "    temperature = 0,    # where have we met this before ? ü§î\n",
        "    task='transcribe') # or translate\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1hJLY6O_PpI",
        "outputId": "03c08521-ebfb-4d8d-ef92-cc1fd0321aa9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sum caminho risulta Isabelle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOdK9a2cBR3f",
        "outputId": "07220fc1-b513-4a4e-c882-aae97c105827"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ' I had that curiosity beside me at this moment.',\n",
              " 'segments': [{'id': 0,\n",
              "   'seek': 0,\n",
              "   'start': 0.0,\n",
              "   'end': 3.2,\n",
              "   'text': ' I had that curiosity beside me at this moment.',\n",
              "   'tokens': [50364,\n",
              "    286,\n",
              "    632,\n",
              "    300,\n",
              "    18769,\n",
              "    15726,\n",
              "    385,\n",
              "    412,\n",
              "    341,\n",
              "    1623,\n",
              "    13,\n",
              "    50524],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2618454419649564,\n",
              "   'compression_ratio': 0.8679245283018868,\n",
              "   'no_speech_prob': 0.01015368103981018}],\n",
              " 'language': 'en'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, so we know how Whisper works now. Can we integrate this into a Gradio demo ?\n"
      ],
      "metadata": {
        "id": "hNS7kvl2Acfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Whisper + Gradio"
      ],
      "metadata": {
        "id": "sO5koMXtBkmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def whisper_transcribe(audio, temp, lang):\n",
        "\n",
        "  if lang == 'Detect':\n",
        "    lang = None\n",
        "\n",
        "  result = model.transcribe(\n",
        "    audio, \n",
        "    language=lang, \n",
        "    temperature=temp,    \n",
        "    task='transcribe')\n",
        "\n",
        "  return result[\"text\"]\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:   # main gradio block, always necessary\n",
        "\n",
        "  # from here we can populate the UI as we wish\n",
        "  gr.Markdown(\"DeepCamp: the best AI camp in the **world**\")\n",
        "  with gr.Row():\n",
        "      temp = gr.Slider(minimum=0, maximum=1)\n",
        "      lang = gr.Radio(choices=['en', 'it', 'pt', 'es', 'fr', 'Detect'], value='Detect')\n",
        "  with gr.Tab(\"Upload\"):\n",
        "      audio_up = gr.Audio(source=\"upload\", type='filepath')\n",
        "      transcr_up = gr.Button('Trascribe')\n",
        "\n",
        "  with gr.Tab(\"Record\"):\n",
        "      audio_mic = gr.Audio(source=\"microphone\", type='filepath')\n",
        "      transcr_mic = gr.Button('Trascribe')\n",
        "  \n",
        "  trascript = gr.Text()\n",
        "  \n",
        "  \n",
        "  transcr_up.click(\n",
        "      whisper_transcribe, \n",
        "      inputs=[audio_up, temp, lang], \n",
        "      outputs=trascript)\n",
        "  \n",
        "  transcr_mic.click(\n",
        "      whisper_transcribe, \n",
        "      inputs=[audio_mic, temp, lang], \n",
        "      outputs=trascript)\n",
        "\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "UQO110GPxzJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Final Exercise üî•\n",
        "\n",
        "Build a gradio demo that predicts the content of an image using a pretrained ResNet."
      ],
      "metadata": {
        "id": "-Cl6eaW9JDOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet34, ResNet34_Weights\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "labels = eval(requests.get('https://raw.githubusercontent.com/alessiodevoto/deepers/main/data/imagenet1000_clsidx_to_labels.txt').text)\n",
        "resnet34 = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
        "resnet34.eval()\n",
        "preprocess = ResNet34_Weights.DEFAULT.transforms()"
      ],
      "metadata": {
        "id": "-4tqoe6OPVa1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "\n",
        "\n",
        "def classify(img):\n",
        "  image = Image.open(img)\n",
        "  img_transformed = preprocess(image)\n",
        "  logits = resnet34(img_transformed.unsqueeze(0))\n",
        "  predictions = torch.nn.functional.softmax(logits, dim=1)[0] \n",
        "  confidences = {labels[i]: float(predictions[i]) for i in range(1000)}\n",
        "  return confidences\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:   # main gradio block, always necessary\n",
        "  gr.Markdown(\"DeepCamp: the best AI camp in the **world**\")\n",
        "  \n",
        "  with gr.Row():\n",
        "      input_image = gr.Image(type='filepath')\n",
        "      pred = gr.Label(num_top_classes=3)\n",
        "  with gr.Row():\n",
        "    pred_btn = gr.Button('Classify')\n",
        "\n",
        "  pred_btn.click(\n",
        "      classify, \n",
        "      inputs=input_image, \n",
        "      outputs=pred)\n",
        "  \n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "du8C2CfkOGtE",
        "outputId": "7a89d9be-b50e-453e-a6f6-40b9f94f06c6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQA3_rLCPzx9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}