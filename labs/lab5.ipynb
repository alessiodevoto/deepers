{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8seNS9m74E7"
      },
      "source": [
        "# Deepcamp: Codelab 5\n",
        "\n",
        "**In this tutorial we will cover**:\n",
        "\n",
        "- Deployment of apps based on ML or DL models\n",
        "\n",
        "\n",
        "**Author**:\n",
        "- Alessio Devoto (alessio.devoto@uniroma1.it)\n",
        "\n",
        "\n",
        "**Duration**: 50 mins \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmixIcfvEMNO"
      },
      "source": [
        "# Deployment of Machine Learning models \n",
        "\n",
        "Let's say you designed and trained a very cool and effective deep learning model. \n",
        "\n",
        "The model is now ready to be tested on a different data and users. What will you do ? \n",
        "\n",
        "- You could get in touch with someone who takes care of designing the infrastructure for your system âž¡ expensive and time consuming\n",
        "- You can use a framework for ML apps deployment and start it on any server (of course, you must own a server)\n",
        "\n",
        "---\n",
        "\n",
        "There are a few libraries we can use for this purpose: [Gradio](https://gradio.app/quickstart/), [StreamLit](https://streamlit.io/) and even come with UI features. If you want a lower level control (at the cose of more complexity), you can use [FastApi](https://fastapi.tiangolo.com/) or [Flask](https://flask.palletsprojects.com/en/2.3.x/).\n",
        "\n",
        "Today, we are going to use Gradio, a new library developed by Huggingface for showcasing their own models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr2Vuosf_xl8"
      },
      "source": [
        "Install necessary libraries as usual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUEr_HyP7us7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U openai-whisper\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests           \n",
        "from PIL import Image         # to deal with images\n",
        "import torch"
      ],
      "metadata": {
        "id": "fu8EdC2VzZLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWmsmzZF_3W9"
      },
      "source": [
        "## 1. Gradio basics\n",
        "\n",
        "Gradio lets you create intuitive User Interfaces in plain Python, by simply listing panels, buttons, text areas etc...\n",
        "\n",
        "All [components](https://gradio.app/docs/#components) must be initialized inside a gradio.Block, which represents an area of the interface.\n",
        "\n",
        "Let's start with a simple example taken from [here](https://gradio.app/docs/#components).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k71aoJ5lIOLt"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:   # main gradio block, always necessary\n",
        "\n",
        "    # from here we can populate the UI as we wish\n",
        "    gr.Markdown(\"DeepCamp: the best place in the **world**\")\n",
        "    \n",
        "    with gr.Tab(label=\"Flip Text\"):\n",
        "        text_input = gr.Textbox()\n",
        "        text_output = gr.Textbox()\n",
        "        text_button = gr.Button(\"Flip\")\n",
        "   \n",
        "    with gr.Tab(label=\"Flip Image\"):\n",
        "        with gr.Row():\n",
        "            image_input = gr.Image()\n",
        "            image_output = gr.Image()\n",
        "        image_button = gr.Button(\"Flip\")\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqihcJ9AOTQ-"
      },
      "source": [
        "Of course, nothing happens if we click the buttons, as there is no function implemented yet.\n",
        "\n",
        "Let's add simple functions which manipulate the input data and return an output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bFZuhnkII7X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "# a function that flips a string\n",
        "def flip_text(x):\n",
        "    return x[::-1]\n",
        "\n",
        "# a function that flips an image\n",
        "def flip_image(x):\n",
        "    return np.fliplr(x)\n",
        "\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:   # main gradio block, always necessary\n",
        "\n",
        "    # from here we can populate the UI as we wish\n",
        "    gr.Markdown(\"DeepCamp: the best place in the **world**\")\n",
        "    \n",
        "    with gr.Tab(label=\"Flip Text\"):\n",
        "        text_input = gr.Textbox()\n",
        "        text_output = gr.Textbox()\n",
        "        text_button = gr.Button(\"Flip\")\n",
        "   \n",
        "    with gr.Tab(label=\"Flip Image\"):\n",
        "        with gr.Row():\n",
        "            image_input = gr.Image()\n",
        "            image_output = gr.Image()\n",
        "        image_button = gr.Button(\"Flip\")\n",
        "\n",
        "    # assign corresponding function to each button\n",
        "    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n",
        "\n",
        "    # notice that the image is passed to the function as a numpy array\n",
        "    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n",
        "\n",
        "demo.launch() # you can control the ports and the number of concurrent threads here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgGIM-30PokF"
      },
      "source": [
        "### Exercise ðŸ‹: write a basic gradio demo\n",
        "\n",
        "Write a gradio demo that given an image blurs it based on the input value provided by the user.\n",
        "\n",
        "For the blur, use the provided function \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "gaussian_filter(image, sigma)\n",
        "```\n",
        "\n",
        "where `image` is the image and `sigma` is the amount of blur.\n",
        "\n",
        "Allow the user to select the amount of blur in the range (0,5) via a gradio Slider (see [here](https://gradio.app/docs/#slider-header))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAb5jmmTNRY6"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "# your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9tQL6n6LK-rX"
      },
      "outputs": [],
      "source": [
        "#@title Peek solution ðŸ‘€\n",
        "\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "\n",
        "def blur(image, amount):\n",
        "  return gaussian_filter(image, sigma=amount)\n",
        "\n",
        "with gr.Blocks() as demo:   # main gradio block, always necessary\n",
        "\n",
        "  # from here we can populate the UI as we wish\n",
        "  gr.Markdown(\"DeepCamp: the best place in the **world**\")\n",
        "\n",
        "  blur_amount = gr.Slider(minimum=0, maximum=5)\n",
        "  with gr.Row():\n",
        "      image_input = gr.Image()\n",
        "      image_output = gr.Image()\n",
        "  blur_btn = gr.Button('Blur')\n",
        "\n",
        "  # note how we pass the params\n",
        "  blur_btn.click(blur, inputs=[image_input, blur_amount], outputs=image_output)\n",
        "\n",
        "\n",
        "demo.launch()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cl6eaW9JDOj"
      },
      "source": [
        "\n",
        "## 2. Gradio Demo 2.0\n",
        "\n",
        "Now that we have an idea of how Gradio works, let's place a real Neural Network behind the UI.\n",
        "\n",
        "**Goal**: write a Gradio demo that exposes a *pretrained* ResNet34 for image classification. \n",
        "\n",
        "ResNet34 is pretrained on 1000 images of the ImageNet dataset, so it will be able to classify 1000 different classes. We can get the pretrained weights from Pytorch (as we saw in lab 3).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4tqoe6OPVa1"
      },
      "outputs": [],
      "source": [
        "# download the list of imagenet classes and store it in a python dictionary\n",
        "labels = eval(requests.get('https://raw.githubusercontent.com/alessiodevoto/deepers/main/data/imagenet1000_clsidx_to_labels.txt').text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet34, ResNet34_Weights\n",
        "\n",
        "# get the model with the pretrained weights\n",
        "resnet34 = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
        "resnet34.eval()     # why ?\n",
        "\n",
        "# transforms \n",
        "preprocess = ResNet34_Weights.DEFAULT.transforms()"
      ],
      "metadata": {
        "id": "IuEMqv2GzoYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function that given an image returns a dictionary where keys are the labels and values are prediction scores."
      ],
      "metadata": {
        "id": "OeRWh9MB2wEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(img):\n",
        "  # open the image\n",
        "  image = Image.open(img)\n",
        "  # apply transforms\n",
        "  img_transformed = preprocess(image)\n",
        "  # get unnormalized scores\n",
        "  logits = resnet34(img_transformed.unsqueeze(0))\n",
        "  predictions = torch.nn.functional.softmax(logits, dim=1)[0] \n",
        "  # create dictionary\n",
        "  confidences = {labels[i]: float(predictions[i]) for i in range(1000)}\n",
        "  return confidences\n"
      ],
      "metadata": {
        "id": "1Jqm3Emc2vdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, our demo"
      ],
      "metadata": {
        "id": "0xfQGLP13EUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "du8C2CfkOGtE",
        "outputId": "7a89d9be-b50e-453e-a6f6-40b9f94f06c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, width, height, cache, element) => {\n                        if (!google.colab.kernel.accessAllowed && !cache) {\n                            return;\n                        }\n                        element.appendChild(document.createTextNode(''));\n                        const url = await google.colab.kernel.proxyPort(port, {cache});\n\n                        const external_link = document.createElement('div');\n                        external_link.innerHTML = `\n                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n                                    https://localhost:${port}${path}\n                                </a>\n                            </div>\n                        `;\n                        element.appendChild(external_link);\n\n                        const iframe = document.createElement('iframe');\n                        iframe.src = new URL(path, url).toString();\n                        iframe.height = height;\n                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n                        iframe.width = width;\n                        iframe.style.border = 0;\n                        element.appendChild(iframe);\n                    })(7860, \"/\", \"100%\", 500, false, window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "with gr.Blocks() as demo:   # main gradio block, always necessary\n",
        "  gr.Markdown(\"DeepCamp: the best AI camp in the **world**\")\n",
        "  \n",
        "  with gr.Row():\n",
        "      input_image = gr.Image(type='filepath')\n",
        "      pred = gr.Label(num_top_classes=3)\n",
        "  with gr.Row():\n",
        "    pred_btn = gr.Button('Classify')\n",
        "  \n",
        "  pred_btn.click(\n",
        "      classify, \n",
        "      inputs=input_image, \n",
        "      outputs=pred)\n",
        "  \n",
        "demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sZd_8qPvCpL"
      },
      "source": [
        "## 3. Final Exercise: Speech to Text ðŸ”¥\n",
        "\n",
        "**Scenario:** we developed an automatic speech recognition (ASR) model which is able to trascribe human speech, and we want to make this service available to the world.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN13saDMDXtn"
      },
      "source": [
        "### 3.1 Meet Whisper\n",
        "Meet OpenAI's Whisper!\n",
        "\n",
        "![whisper](https://openaicom.imgix.net/d9c13138-366f-49d3-b8bd-cb3f5a973a5b/asr-summary-of-model-architecture-desktop.svg?fm=auto&auto=compress,format&fit=min&w=1919&h=1551)\n",
        "\n",
        "\n",
        "[Whisper](https://arxiv.org/pdf/2212.04356.pdf) is an automatic speech recognition (ASR) system trained on *680.000 hours of multilingual and multitask* supervised data collected from the web ðŸ¤¯. \n",
        "\n",
        "The Whisper architecture is a simple end-to-end approach, implemented as an encoder-decoder Transformer. As you may know, Encoder and Decoder blocks are neural networks based on the attention mechanism. \n",
        "\n",
        "As you can see on the [official repository](https://github.com/openai/whisper) they trained the model in different \"sizes\":` tiny, small, base, medium, large, large-v2.`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by downloading an audio sample and listen to it."
      ],
      "metadata": {
        "id": "sJ7BwrgGyCQ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqnVznin_pcy"
      },
      "outputs": [],
      "source": [
        "from torchaudio.utils import download_asset\n",
        "import IPython\n",
        "\n",
        "speech_file = download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")\n",
        "IPython.display.Audio(speech_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can the ***tiny*** Whisper version transcribe this? "
      ],
      "metadata": {
        "id": "Bglafai7yDfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1hJLY6O_PpI",
        "outputId": "03c08521-ebfb-4d8d-ef92-cc1fd0321aa9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/whisper/transcribe.py:114: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Sum caminho risulta Isabelle\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "\n",
        "# the model is now download locally (that's why we picked the smallest version) \n",
        "\n",
        "model = whisper.load_model(\"tiny\") # we load the tiny version\n",
        "\n",
        "result = model.transcribe(\n",
        "    speech_file,        # this can be a path to a file or a numpy array\n",
        "    language='en',      # a lot of languages available\n",
        "    temperature = 0,    # where have we met this before ? ðŸ¤”\n",
        "    task='transcribe')  # or 'translate'\n",
        "\n",
        "print(result[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOdK9a2cBR3f",
        "outputId": "07220fc1-b513-4a4e-c882-aae97c105827"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': ' I had that curiosity beside me at this moment.',\n",
              " 'segments': [{'id': 0,\n",
              "   'seek': 0,\n",
              "   'start': 0.0,\n",
              "   'end': 3.2,\n",
              "   'text': ' I had that curiosity beside me at this moment.',\n",
              "   'tokens': [50364,\n",
              "    286,\n",
              "    632,\n",
              "    300,\n",
              "    18769,\n",
              "    15726,\n",
              "    385,\n",
              "    412,\n",
              "    341,\n",
              "    1623,\n",
              "    13,\n",
              "    50524],\n",
              "   'temperature': 0.0,\n",
              "   'avg_logprob': -0.2618454419649564,\n",
              "   'compression_ratio': 0.8679245283018868,\n",
              "   'no_speech_prob': 0.01015368103981018}],\n",
              " 'language': 'en'}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a bunch of additional fields\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNS7kvl2Acfl"
      },
      "source": [
        "### Whisper + Gradio\n",
        "\n",
        "Ok, so we know how Whisper works now. Can we integrate this into a Gradio demo ?\n",
        "\n",
        "The demo should \n",
        "\n",
        "- allow the user to upload an audio file or record from microphone (see `gradio.Audio`)\n",
        "- allow the user to pick language and task (see `gradio.Radio`) and temperature (see `gr.Slider`)\n",
        "- show the transcription \n",
        "\n",
        "Once you are done\n",
        "- try to record some samples and decode them with different temperatures.\n",
        "- Try to force the transcription into a wrong language.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "gf_CiDWg5mvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQO110GPxzJs"
      },
      "outputs": [],
      "source": [
        "#@title Peek solution ðŸ‘€\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def whisper_transcribe(audio, temp, lang):\n",
        "\n",
        "  if lang == 'Detect':\n",
        "    lang = None\n",
        "\n",
        "  result = model.transcribe(\n",
        "    audio, \n",
        "    language=lang, \n",
        "    temperature=temp,    \n",
        "    task='transcribe')\n",
        "\n",
        "  return result[\"text\"]\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:   # main gradio block, always necessary\n",
        "\n",
        "  # from here we can populate the UI as we wish\n",
        "  gr.Markdown(\"DeepCamp: the best AI camp in the **world**\")\n",
        "  with gr.Row():\n",
        "      temp = gr.Slider(minimum=0, maximum=1)\n",
        "      lang = gr.Radio(choices=['en', 'it', 'pt', 'es', 'fr', 'Detect'], value='Detect')\n",
        "  with gr.Tab(\"Upload\"):\n",
        "      audio_up = gr.Audio(source=\"upload\", type='filepath')\n",
        "      transcr_up = gr.Button('Trascribe')\n",
        "\n",
        "  with gr.Tab(\"Record\"):\n",
        "      audio_mic = gr.Audio(source=\"microphone\", type='filepath')\n",
        "      transcr_mic = gr.Button('Trascribe')\n",
        "  \n",
        "  trascript = gr.Text()\n",
        "  \n",
        "  \n",
        "  transcr_up.click(\n",
        "      whisper_transcribe, \n",
        "      inputs=[audio_up, temp, lang], \n",
        "      outputs=trascript)\n",
        "  \n",
        "  transcr_mic.click(\n",
        "      whisper_transcribe, \n",
        "      inputs=[audio_mic, temp, lang], \n",
        "      outputs=trascript)\n",
        "\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}